{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fa76f1",
   "metadata": {},
   "source": [
    "### ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f09fc7",
   "metadata": {},
   "source": [
    "### WEB SCRAPING – ASSIGNMENT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6e1e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea135bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e4c099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.8.3-py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rabi\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed async-generator-1.10 exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.8.3 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376cb4a",
   "metadata": {},
   "source": [
    "#### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "094a3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32224c0",
   "metadata": {},
   "source": [
    "Now we will download the webDriver for the web Browser Step for download are- \n",
    "1.Check the version of your browser \n",
    "2.go to the link https://chromedriver.chromium.org/downloads \n",
    "3.Download the webdriver for your version of your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2e390fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "082734a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6256693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aca0b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input\")\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a69bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a3b5db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72f936e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "# scraping job Experience from the given page\n",
    "\n",
    "experience_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience_tags[0:10]:\n",
    "    exp=i.text\n",
    "    experience_required.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e1df16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(experience_required))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66265e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Engineer/Data Analyst</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Ingersoll Rand</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Unusual Hire</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - Contractual</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Search Advisers Services Guj</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - Contractual</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Search Advisers Services Guj</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ExcelHER-Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Volvo Financial Services</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Clarivate</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Novel Office</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst - EdTech</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Talentstack</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - MySQL</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Talentstack</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Job Title  \\\n",
       "0  Data Engineer/Data Analyst   \n",
       "1                Data Analyst   \n",
       "2                Data Analyst   \n",
       "3  Data Analyst - Contractual   \n",
       "4  Data Analyst - Contractual   \n",
       "5       ExcelHER-Data Analyst   \n",
       "6                Data Analyst   \n",
       "7                Data Analyst   \n",
       "8       Data Analyst - EdTech   \n",
       "9        Data Analyst - MySQL   \n",
       "\n",
       "                                        Job Location  \\\n",
       "0  Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                   Company Name Experience Required  \n",
       "0                 Tech Mahindra            6-11 Yrs  \n",
       "1                Ingersoll Rand             3-6 Yrs  \n",
       "2                  Unusual Hire             1-4 Yrs  \n",
       "3  Search Advisers Services Guj             2-3 Yrs  \n",
       "4  Search Advisers Services Guj             2-3 Yrs  \n",
       "5      Volvo Financial Services             3-5 Yrs  \n",
       "6                     Clarivate             2-4 Yrs  \n",
       "7                  Novel Office             0-2 Yrs  \n",
       "8                   Talentstack             2-6 Yrs  \n",
       "9                   Talentstack             2-7 Yrs  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame({'Job Title':job_title,'Job Location':job_location,'Company Name':company_name,'Experience Required':experience_required})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60eb4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6246c2b0",
   "metadata": {},
   "source": [
    "#### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717e6ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"fee06c43c657691998f9f3b929238bc2\", element=\"94442867-7a36-4f81-ba31-28c898825a72\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"fee06c43c657691998f9f3b929238bc2\", element=\"c172cc6b-f6d1-4533-adce-8dec4bd6813a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"fee06c43c657691998f9f3b929238bc2\", element=\"877e3c32-28f5-4e93-8f4b-32d435495906\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"fee06c43c657691998f9f3b929238bc2\", element=\"0b202f7b-7f69-4ed3-ad3c-0e90d3c127ba\")>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to fetcg the url -\n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "url[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76aa21f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.naukri.com/job-listings-data-engineer-data-analyst-tech-mahindra-kolkata-hyderabad-secunderabad-pune-chennai-bangalore-bengaluru-delhi-ncr-mumbai-all-areas-6-to-11-years-130223001593\n",
      "https://www.naukri.com/job-listings-data-analyst-ingersoll-rand-india-ltd-bangalore-bengaluru-3-to-6-years-310323500515\n",
      "https://www.naukri.com/job-listings-data-analyst-unusual-hire-bangalore-bengaluru-1-to-4-years-181220500115\n",
      "https://www.naukri.com/job-listings-data-analyst-contractual-search-advisers-services-guj-bangalore-bengaluru-2-to-3-years-300323608803\n"
     ]
    }
   ],
   "source": [
    "for i in url[0:4]: # lets provide range to print only top 4 data\n",
    "    \n",
    "    print(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d87a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ddf6ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=2\n",
    "for page in range (start,end):\n",
    "    titles=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "    for i in titles:\n",
    "        job_titles.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"fright fs14 btn-secondary br2\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d40be982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "285e3398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Engineer/Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst - Contractual',\n",
       " 'Data Analyst - Contractual',\n",
       " 'ExcelHER-Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst - EdTech',\n",
       " 'Data Analyst - MySQL',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Procurement Data Analyst - SCM',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Salesforce Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst-PYSPARK',\n",
       " 'Data Analyst',\n",
       " 'STAFF DATA ANALYST',\n",
       " 'data analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data analyst in Python',\n",
       " 'Onsite Opportunity -Data-Analyst',\n",
       " 'DATA ANALYST',\n",
       " 'Onsite Opportunity -Data-Analyst',\n",
       " 'Data analyst in Python',\n",
       " 'DATA ANALYST',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Urgent Hiring || Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Governance Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Sustainability Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'PCN Data Analyst',\n",
       " 'data analyst',\n",
       " 'Data Analyst - Python']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210f167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90790c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b3fe8",
   "metadata": {},
   "source": [
    "Now we will download the webDriver for the web Browser Step for download are- 1.Check the version of your browser 2.go to the link https://chromedriver.chromium.org/downloads 3.Download the webdriver for your version of your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888c7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cc0774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b687f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e756b5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input')\n",
    "location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "826d90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "612c2845",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b033a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0ee63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ced18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Professional - IBM SPSS Statistic...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Mumbai, Pune, Chennai</td>\n",
       "      <td>Hexaware Technologies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior data scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist_NLP</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Machine Learning (AI) Architect</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Persistent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manager - Innovations Hub - Machine Learning</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Engineer Consultant Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Verizon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Engineer Consultant-Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Verizon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Data Science Professional - IBM SPSS Statistic...   \n",
       "1                            Data Science Specialist   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3                              Senior data scientist   \n",
       "4                                 Data Scientist_NLP   \n",
       "5                                     Data Scientist   \n",
       "6                    Machine Learning (AI) Architect   \n",
       "7       Manager - Innovations Hub - Machine Learning   \n",
       "8            Senior Engineer Consultant Data Science   \n",
       "9            Senior Engineer Consultant-Data Science   \n",
       "\n",
       "                                        Job Location           Company Name  \n",
       "0  Bangalore/Bengaluru, Noida, Mumbai, Pune, Chennai  Hexaware Technologies  \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...              Accenture  \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...              Accenture  \n",
       "3  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics  \n",
       "4  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics  \n",
       "5  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics  \n",
       "6  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...             Persistent  \n",
       "7  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...                    PwC  \n",
       "8                                Bangalore/Bengaluru                Verizon  \n",
       "9                                Bangalore/Bengaluru                Verizon  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame({'Job Title':job_title,'Job Location':job_location,'Company Name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b23b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ce7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8872ae32",
   "metadata": {},
   "source": [
    "#### Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1128eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ca2b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8ae8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be67f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering designation and location as required in the question-\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fffee454",
   "metadata": {},
   "outputs": [],
   "source": [
    "location=driver.find_element(By.XPATH,'/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input')\n",
    "location.send_keys('Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1088dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb0fbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary=driver.find_element(By.XPATH,\"/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5bd2bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "517afe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping job title from the given page\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    job_title.append(title)\n",
    "    \n",
    "# scraping job location from the given page\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[0:10]:\n",
    "    location=i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "# scraping company name from the given page\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[0:10]:\n",
    "    company=i.text\n",
    "    company_name.append(company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "059beca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "092b18f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...</td>\n",
       "      <td>Accenture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr. Data scientist</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "      <td>PayU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Temp. WFH - Kochi/Cochin, Kolkata, Hyderabad/S...</td>\n",
       "      <td>Cognizant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist- Bangalore</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Trigent Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "      <td>Analytos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Truecaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Blackbuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MACHINE LEARNING / DATA SCIENCE TRAINER</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Ethnotech Academy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist/Sr. Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Synechron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Software Engineer Staff</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Juniper Networks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Job Title  \\\n",
       "0                  Data Science Specialist   \n",
       "1                       Sr. Data scientist   \n",
       "2                           Data Scientist   \n",
       "3                Data Scientist- Bangalore   \n",
       "4                    Junior Data Scientist   \n",
       "5                    Senior Data Scientist   \n",
       "6                           Data Scientist   \n",
       "7  MACHINE LEARNING / DATA SCIENCE TRAINER   \n",
       "8        Data Scientist/Sr. Data Scientist   \n",
       "9                  Software Engineer Staff   \n",
       "\n",
       "                                        Job Location       Company Name  \n",
       "0  Kolkata, Mumbai, Hyderabad/Secunderabad, Pune,...          Accenture  \n",
       "1   Hybrid - Bangalore/Bengaluru, Mumbai (All Areas)               PayU  \n",
       "2  Temp. WFH - Kochi/Cochin, Kolkata, Hyderabad/S...          Cognizant  \n",
       "3                                Bangalore/Bengaluru   Trigent Software  \n",
       "4  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...           Analytos  \n",
       "5                                Bangalore/Bengaluru         Truecaller  \n",
       "6              Gurgaon/Gurugram, Bangalore/Bengaluru          Blackbuck  \n",
       "7                                Bangalore/Bengaluru  Ethnotech Academy  \n",
       "8                                Bangalore/Bengaluru          Synechron  \n",
       "9                                Bangalore/Bengaluru   Juniper Networks  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame({'Job Title':job_title,'Job Location':job_location,'Company Name':company_name})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1260f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929696b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c3a650e",
   "metadata": {},
   "source": [
    "#### Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc15d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cfec4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddf58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the Flipkart page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a6d2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sunglasses as required in the question-\n",
    "\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8b03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2cd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_title=[]\n",
    "Product_Description=[]\n",
    "Price_title=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bcc3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sunglasses from the given page\n",
    "\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:20]:\n",
    "    brand=i.text\n",
    "    Brand_title.append(brand)\n",
    "    \n",
    "# scraping Product_Description from the given page\n",
    "\n",
    "Product_tags=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in Product_tags[0:20]:\n",
    "    product=i.text\n",
    "    Product_Description.append(product)\n",
    "    \n",
    "# scraping Price name from the given page\n",
    "\n",
    "Price_tags=driver.find_elements(By.XPATH,'//a[@class=\"_3bPFwb\"]')\n",
    "for i in Price_tags[0:20]:\n",
    "    price=i.text\n",
    "    Price_title.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce86e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(Brand_title),len(Product_Description),len(Price_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9adc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_Description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df12feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=3\n",
    "for page in range (start,end):\n",
    "    titles=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in titles:\n",
    "        Product_Description.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36216b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Product_Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b83b5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (50)',\n",
       " 'UV Protection Cat-eye, Retro Square, Oval, Round Sungla...',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator, Wayfarer Sunglasses (54)',\n",
       " 'UV Protection, Mirrored Wayfarer Sunglasses (54)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Clubmaster Sunglasses (54)',\n",
       " 'UV Protection Oval Sunglasses (54)',\n",
       " 'UV Protection Wayfarer Sunglasses (50)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (53)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'Polarized Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Rectangular Sunglasses (Free Size)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection, Polarized, Mirrored Rectangular Sunglass...',\n",
       " 'UV Protection Round Sunglasses (53)',\n",
       " 'Toughened Glass Lens, UV Protection Aviator Sunglasses ...',\n",
       " 'UV Protection Sports Sunglasses (65)',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'Gradient Rectangular Sunglasses (Free Size)',\n",
       " 'UV Protection Wayfarer Sunglasses (57)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (56)',\n",
       " 'Others Retro Square Sunglasses (58)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection, Mirrored Wayfarer Sunglasses (57)',\n",
       " 'UV Protection Round Sunglasses (53)',\n",
       " 'Gradient Rectangular Sunglasses (Free Size)',\n",
       " 'UV Protection Shield Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'UV Protection Rectangular Sunglasses (52)',\n",
       " 'UV Protection Rectangular, Retro Square Sunglasses (58)',\n",
       " 'UV Protection Clubmaster Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (50)',\n",
       " 'UV Protection Cat-eye, Retro Square, Oval, Round Sungla...',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator, Wayfarer Sunglasses (54)',\n",
       " 'UV Protection, Mirrored Wayfarer Sunglasses (54)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Clubmaster Sunglasses (54)',\n",
       " 'UV Protection Oval Sunglasses (54)',\n",
       " 'UV Protection Wayfarer Sunglasses (50)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (53)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'Polarized Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Rectangular Sunglasses (Free Size)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection, Polarized, Mirrored Rectangular Sunglass...',\n",
       " 'UV Protection Round Sunglasses (53)',\n",
       " 'Toughened Glass Lens, UV Protection Aviator Sunglasses ...',\n",
       " 'UV Protection Sports Sunglasses (65)',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'Gradient Rectangular Sunglasses (Free Size)',\n",
       " 'UV Protection Wayfarer Sunglasses (57)',\n",
       " 'UV Protection Aviator Sunglasses (58)',\n",
       " 'UV Protection Wayfarer Sunglasses (56)',\n",
       " 'Others Retro Square Sunglasses (58)',\n",
       " 'Gradient, UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection, Mirrored Wayfarer Sunglasses (57)',\n",
       " 'UV Protection Round Sunglasses (53)',\n",
       " 'Gradient Rectangular Sunglasses (Free Size)',\n",
       " 'UV Protection Shield Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'UV Protection Rectangular Sunglasses (52)',\n",
       " 'UV Protection Rectangular, Retro Square Sunglasses (58)',\n",
       " 'UV Protection Aviator Sunglasses (Free Size)',\n",
       " 'UV Protection Rectangular Sunglasses (52)',\n",
       " 'UV Protection Retro Square Sunglasses (54)',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Wayfarer Sunglasses (53)',\n",
       " 'Polarized, UV Protection, Riding Glasses Wayfarer Sungl...',\n",
       " 'UV Protection, Polarized Wayfarer Sunglasses (Free Size...',\n",
       " 'Mirrored, UV Protection Rectangular, Retro Square, Wayf...',\n",
       " 'UV Protection Over-sized Sunglasses (60)',\n",
       " 'UV Protection Rectangular Sunglasses (Free Size)',\n",
       " 'UV Protection Cat-eye, Retro Square, Oval, Round Sungla...',\n",
       " 'UV Protection, Polarized, Mirrored, Riding Glasses Wayf...',\n",
       " 'UV Protection, Mirrored Aviator Sunglasses (57)',\n",
       " 'UV Protection Wayfarer Sunglasses (54)',\n",
       " 'UV Protection Aviator Sunglasses (50)',\n",
       " 'UV Protection Retro Square Sunglasses (Free Size)',\n",
       " 'UV Protection Wayfarer Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (54)',\n",
       " 'UV Protection Rectangular Sunglasses (Free Size)',\n",
       " 'UV Protection Aviator Sunglasses (57)',\n",
       " 'UV Protection Rectangular Sunglasses (56)',\n",
       " 'Mirrored, Night Vision, UV Protection, Riding Glasses S...',\n",
       " 'UV Protection Round Sunglasses (Free Size)',\n",
       " 'UV Protection, Riding Glasses Sports Sunglasses (Free S...']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Product_Description[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c6b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "889d4329",
   "metadata": {},
   "source": [
    "#### Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/\n",
    "itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b3e4146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f4342ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a950a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the Flipkart page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/p/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART&q=i+phone+11+64bg&store=tyy%2F4io&srno=s_1_2&otracker=AS_QueryStore_OrganicAutoSuggest_1_11_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_11_na_na_na&fm=organic&iid=70e4e5af-9da5-4d5c-8116-262118582a21.MOBFWQ6BXGJCEYNY.SEARCH&ppt=hp&ppn=homepage&ssid=0arncw6a8w0000001680870132713&qH=20a00a8386a410d5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b85d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating=[]\n",
    "Review_summary=[]\n",
    "Full_review=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52d3a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sunglasses from the given page\n",
    "\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "for i in rating_tags[0:10]:\n",
    "    rating=i.text\n",
    "    Rating.append(rating)\n",
    "    \n",
    "# scraping Product_Description from the given page\n",
    "\n",
    "Review_tags=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "for i in Review_tags[0:10]:\n",
    "    review=i.text\n",
    "    Review_summary.append(review)\n",
    "    \n",
    "# scraping Price name from the given page\n",
    "\n",
    "fullreview_tags=driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "for i in fullreview_tags[0:10]:\n",
    "    fullreview=i.text\n",
    "    Full_review.append(fullreview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b60ec38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Review_summary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba114123",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=4\n",
    "for page in range (start,end):\n",
    "    review=driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for i in review:\n",
    "        Review_summary.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53b3b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "746dcae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Simply awesome',\n",
       " 'Perfect product!',\n",
       " 'Best in the market!',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Simply awesome',\n",
       " 'Perfect product!',\n",
       " 'Best in the market!',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Pretty good',\n",
       " 'Highly recommended',\n",
       " 'Great product',\n",
       " 'Fabulous!',\n",
       " 'Classy product',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Good choice',\n",
       " 'Perfect product!',\n",
       " 'Highly recommended',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Perfect product!',\n",
       " 'Simply awesome',\n",
       " 'Perfect product!',\n",
       " 'Best in the market!',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Pretty good',\n",
       " 'Highly recommended',\n",
       " 'Great product',\n",
       " 'Fabulous!',\n",
       " 'Classy product',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Good choice',\n",
       " 'Perfect product!',\n",
       " 'Highly recommended',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Perfect product!',\n",
       " 'Simply awesome',\n",
       " 'Perfect product!',\n",
       " 'Best in the market!',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Pretty good',\n",
       " 'Highly recommended',\n",
       " 'Great product',\n",
       " 'Simply awesome',\n",
       " 'Perfect product!',\n",
       " 'Best in the market!',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Pretty good',\n",
       " 'Highly recommended',\n",
       " 'Great product',\n",
       " 'Fabulous!',\n",
       " 'Classy product',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Good choice',\n",
       " 'Perfect product!',\n",
       " 'Highly recommended',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Perfect product!',\n",
       " 'Simply awesome',\n",
       " 'Perfect product!',\n",
       " 'Best in the market!',\n",
       " 'Value-for-money',\n",
       " 'Highly recommended',\n",
       " 'Worth every penny',\n",
       " 'Perfect product!',\n",
       " 'Pretty good',\n",
       " 'Highly recommended',\n",
       " 'Great product']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Review_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb746b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba801951",
   "metadata": {},
   "source": [
    "#### Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff4462ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fbde59df",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6a4930e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the Flipkart page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00ff8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering sneakers as required in the question-\n",
    "\n",
    "product=driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "product.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1fee494",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.CLASS_NAME,\"_34RNph\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b596d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand_title=[]\n",
    "Product_Description=[]\n",
    "Price_title=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1bd46e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping sunglasses from the given page\n",
    "\n",
    "brand_tags=driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "for i in brand_tags[0:10]:\n",
    "    brand=i.text\n",
    "    Brand_title.append(brand)\n",
    "    \n",
    "# scraping Product_Description from the given page\n",
    "\n",
    "Product_tags=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "for i in Product_tags[0:10]:\n",
    "    product=i.text\n",
    "    Product_Description.append(product)\n",
    "    \n",
    "# scraping Price name from the given page\n",
    "\n",
    "Price_tags=driver.find_elements(By.XPATH,'//a[@class=\"_3bPFwb\"]')\n",
    "for i in Price_tags[0:10]:\n",
    "    price=i.text\n",
    "    Price_title.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6387401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_Description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f1bd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=4\n",
    "for page in range (start,end):\n",
    "    product=driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in product:\n",
    "        Product_Description.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"_1LKTO3\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54ceebf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Product_Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c75340e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['574 Sneakers For Men',\n",
       " '500 Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Combo Pack Of 2 Casual Shoes Sneakers For Men',\n",
       " 'Synthetic| Lightweight| Premiun| Comfort| Summer Tendy|...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Combo Pack of 2 Casual Shoes Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Combo Pack of 2 Sports Sneakers For Men',\n",
       " 'Premium Casual Shoes for Women Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Exclusive Affordable Collection of Trendy & Styl...',\n",
       " 'Premium Sports Shoes For Men Pack Of 2 Sneakers For Men',\n",
       " 'Comfortable Canvas Lace up Casual Sneakers Shoes For Me...',\n",
       " 'Sneakers For Women',\n",
       " 'Combo Pack of 2 Sports Sneakers For Men',\n",
       " 'Casual Shoes Sneakers For Women',\n",
       " 'WATERPROOF-05cFULLWHITE Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " '327 Sneakers For Men',\n",
       " '327 Sneakers For Men',\n",
       " 'Modern Trendy Sneakers boot Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Road Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Premium Casual Shoes for Women Sneakers For Women',\n",
       " 'luxuryfashion fashionable casual sneaker shoes white Sn...',\n",
       " 'Modern Trendy Sneakers boot Sneakers Sneakers For Men',\n",
       " 'Puma Smash Vulc Sneakers For Men',\n",
       " '574 Sneakers For Men',\n",
       " '373 Sneakers For Men',\n",
       " 'Thunder-01 White Canvas, Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Casual Sneaker High Top Shoes for Walking/Running/Gymwe...',\n",
       " 'Stylish Casual Sports Shoe Sneakers Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Puma Wired Cage Sneakers For Men',\n",
       " 'Casual Sneakers White Outdoor Shoes For Boys And Men Sn...',\n",
       " 'Premium Casual Shoes for Women Sneakers For Women',\n",
       " '574 Sneakers For Men',\n",
       " 'Mesh |Lightweight|Comfort|Summer|Trendy|Walking|Outdoor...',\n",
       " 'Stylish & Trending Outdoor Walking Comfortable Sneakers...',\n",
       " 'Sneakers For Men',\n",
       " '5740 Sneakers For Men',\n",
       " 'Mesh |Lightweight|Comfort|Summer|Trendy|Walking|Outdoor...',\n",
       " 'Stylish Comfortable Lightweight, Breathable Women Shoes...',\n",
       " 'Modern Trendy Shoes Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Lightweight,Comfort,Summer,Trendy,Walking,Outdoor,Styli...',\n",
       " 'Comfortable Canvas Lace up Casual Sneakers Shoes For Me...',\n",
       " 'Sneakers For Men',\n",
       " 'Bersache Lightweight Color Changing Casual,Sneaker Shoe...',\n",
       " 'Sneakers For Men',\n",
       " 'Carnival-02 Mens High Top Casual Chunky Sneakers Sneake...',\n",
       " 'Comfy Mid-Top Casual Chunky Streetwear Fashion Sneakers...',\n",
       " 'Stylish & Trending Outdoor Walking Comfortable Sneakers...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Premium White Sneakers For women Sneakers For Women',\n",
       " 'SS1100 Sneakers For Men',\n",
       " 'Advantage Genz Sneakers For Men Sneakers For Men',\n",
       " 'Latest Exclusive Affordable Collection of Trendy & Styl...',\n",
       " 'Stylish Sneakers Shoes for Women And Girls Sneakers For...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'kardam&sons luxury fashionable Stylish Light Weight Sne...',\n",
       " 'Premium Casual Shoes for Women Sneakers For Women',\n",
       " 'Premium Casual Shoes for Women Sneakers For Women',\n",
       " '500 Sneakers For Men',\n",
       " 'Combo Pack of 2 Casual Shoe Sneakers For Men',\n",
       " 'Lightweight,Comfort,Summer,Trendy,Walking,Outdoor,Styli...',\n",
       " '574 Sneakers For Men',\n",
       " '500 Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Women',\n",
       " 'Combo Pack Of 2 Casual Shoes Sneakers For Men',\n",
       " 'Synthetic| Lightweight| Premiun| Comfort| Summer Tendy|...',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Combo Pack of 2 Casual Shoes Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Combo Pack of 2 Sports Sneakers For Men',\n",
       " 'Premium Casual Shoes for Women Sneakers For Women',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Sneakers For Men',\n",
       " 'Latest Exclusive Affordable Collection of Trendy & Styl...',\n",
       " 'Premium Sports Shoes For Men Pack Of 2 Sneakers For Men',\n",
       " 'Comfortable Canvas Lace up Casual Sneakers Shoes For Me...',\n",
       " '327 Sneakers For Men']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Product_Description[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b070a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a33b5546",
   "metadata": {},
   "source": [
    "#### Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a06018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4987e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91cd0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the amazon page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6347780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f388d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering laptop as required in the question-\n",
    "\n",
    "product=driver.find_element(By.ID,\"twotabsearchtextbox\")\n",
    "\n",
    "product.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52752fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.ID,\"nav-search-submit-button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23134b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu=driver.find_element(By.XPATH,\"//span[text()='Intel Core i7']\")\n",
    "cpu.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd05f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Ratings_title=[]\n",
    "Price_title=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c83e213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping laptop from the given page\n",
    "\n",
    "title_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "for i in title_tags[0:10]:\n",
    "    title=i.text\n",
    "    Title.append(title)\n",
    "    \n",
    "# scraping Product_Description from the given page\n",
    "\n",
    "rating_tags=driver.find_elements(By.XPATH,'//div[@class=\"a-row a-size-small\"]')\n",
    "for i in rating_tags[0:10]:\n",
    "    rating=i.text\n",
    "    Ratings_title.append(rating)\n",
    "    \n",
    "# scraping Price name from the given page\n",
    "\n",
    "Price_tags=driver.find_elements(By.XPATH,'//span[@class=\"a-price-whole\"]')\n",
    "for i in Price_tags[0:10]:\n",
    "    price=i.text\n",
    "    Price_title.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c88676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(Title),len(Ratings_title),len(Price_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0405326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5685eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fa39157",
   "metadata": {},
   "source": [
    "#### Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpage https://www.azquotes.com/\n",
    "2. Click on Top Quotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f29d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5aa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Rabi\\flip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd0bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the azquotes page on automated chorme browser\n",
    "\n",
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9f19ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc68bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_quotes=driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\")\n",
    "top_quotes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3d7aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quote=[]\n",
    "Author=[]\n",
    "Type_Of_Quotes=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30da0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Quote from the given page\n",
    "\n",
    "quote_tags=driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "for i in quote_tags:\n",
    "    quote=i.text\n",
    "    Quote.append(quote)\n",
    "    \n",
    "# scraping Author from the given page\n",
    "\n",
    "author_tags=driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "for i in author_tags:\n",
    "    author=i.text\n",
    "    Author.append(author)\n",
    "    \n",
    "# scraping Type_Of_Quotes name from the given page\n",
    "\n",
    "type_Of_Quotes_tags=driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "for i in type_Of_Quotes_tags:\n",
    "    type_Of_Quotes=i.text\n",
    "    Type_Of_Quotes.append(type_Of_Quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a0eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Quote),len(Author),len(Type_Of_Quotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb352e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Type_Of_Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>When the going gets weird, the weird turn pro.</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Music, Sports, Hunting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>When a train goes through a tunnel and it gets...</td>\n",
       "      <td>Corrie Ten Boom</td>\n",
       "      <td>Trust, Encouraging, Uplifting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>If you think you are too small to make a diffe...</td>\n",
       "      <td>Dalai Lama</td>\n",
       "      <td>Inspirational, Funny, Change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>God doesn't require us to succeed, he only req...</td>\n",
       "      <td>Mother Teresa</td>\n",
       "      <td>Success, God, Mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Change your thoughts and you change your world.</td>\n",
       "      <td>Norman Vincent Peale</td>\n",
       "      <td>Inspirational, Motivational, Change</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Quote                Author  \\\n",
       "0   The essence of strategy is choosing what not t...        Michael Porter   \n",
       "1   One cannot and must not try to erase the past ...            Golda Meir   \n",
       "2   Patriotism means to stand by the country. It d...    Theodore Roosevelt   \n",
       "3   Death is something inevitable. When a man has ...        Nelson Mandela   \n",
       "4   You have to love a nation that celebrates its ...          Erma Bombeck   \n",
       "..                                                ...                   ...   \n",
       "95     When the going gets weird, the weird turn pro.    Hunter S. Thompson   \n",
       "96  When a train goes through a tunnel and it gets...       Corrie Ten Boom   \n",
       "97  If you think you are too small to make a diffe...            Dalai Lama   \n",
       "98  God doesn't require us to succeed, he only req...         Mother Teresa   \n",
       "99    Change your thoughts and you change your world.  Norman Vincent Peale   \n",
       "\n",
       "                              Type_Of_Quotes  \n",
       "0   Essence, Deep Thought, Transcendentalism  \n",
       "1                  Inspiration, Past, Trying  \n",
       "2                        Country, Peace, War  \n",
       "3         Inspirational, Motivational, Death  \n",
       "4               4th Of July, Food, Patriotic  \n",
       "..                                       ...  \n",
       "95                    Music, Sports, Hunting  \n",
       "96             Trust, Encouraging, Uplifting  \n",
       "97              Inspirational, Funny, Change  \n",
       "98                      Success, God, Mother  \n",
       "99       Inspirational, Motivational, Change  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame({'Quote':Quote,'Author':Author,'Type_Of_Quotes':Type_Of_Quotes})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52741e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
